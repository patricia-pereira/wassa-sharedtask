# ------------------------  PyTorch Lightning Configurations --------------------------------------
seed: 111 # Training seed set everywhere
verbose: False                            # Verbosity level
experiment_name: '111_task_1_convd_roberta-base'

# ----------------------------- Early Stopping ----------------------------------------------------
monitor: mse                         # Metric to monitor during training
min_delta: 0.0                            # Sensitivity to the metric.
patience: 5                             # Number of epochs without improvement before stopping training
metric_mode: min                          # 'min' or 'max' depending if we wish to maximize or minimize the metric

# ----------------------------- Model Checkpoint --------------------------------------------------
save_top_k: 1                             # How many checkpoints we want to save.
save_weights_only: True                   # Saves the model weights only

# ----------------------------- Lightning Trainer --------------------------------------------------
gradient_clip_val: 1.0                    # Clips gradients when the norm value exceeds 1.0
gpus: 1                                  # Number of GPUs to use. (1 is recommended)
deterministic: True                       # if true enables cudnn.deterministic. Might make your system slower, but ensures reproducibility.
overfit_batches: 0.0                      # DEGUB: Uses this much data of the training set. If nonzero, will use the same training set for validation and testing.
accumulate_grad_batches: -1                # Gradient accumulation steps
min_epochs: 1                             # Min number of epochs
max_epochs: 40                            # Max number of epochs

# --------------------------------- Dataset -------------------------------------------------------

task: 1_convd
pretrained_model: roberta-base #allenai/longformer-base-4096  # to run MiniBERT set this flag to: google/bert_uncased_L-2_H-128_A-2                                # Options: ekman, goemotions, polarity
batch_size: 4                            # Batch size used during training.

# -------------------------------- Transformer Fine-tuning -----------------------------------------------
nr_frozen_epochs: 1                       # Number of epochs where the encoder model is frozen (can also be a float between 0 and 1).
encoder_learning_rate: 1.0e-5             # Learning rate to be used for the encoder parameters.
learning_rate: 5.0e-5                     # Learning rate to be used on the classification head.
layerwise_decay: 0.95                     # Learning rate dacay for the encoder layers.


context: False
context_turns: 0